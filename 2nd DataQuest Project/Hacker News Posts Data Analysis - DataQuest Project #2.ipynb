{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis of the Hacker News Dataset\n",
    "\n",
    "[Hacker News](https://news.ycombinator.com/) is a website where user-submitted stories (known as \"posts\"). The posts are voted and commented upon, similar to reddit. Hacker News is very popular in technology and startup circles, and posts that make it to the top of Hacker News' listings can get hundreds of thousands of visitors as a result.\n",
    "\n",
    "We will be analyzing posts that start with 'Ask HN' and 'Show HN'. \n",
    "* **Ask HN** is where a user will ask the Hacker News community a question.\n",
    "* **Show HN** is where a user will show the Hacker News community a project, product and just something interesting.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "### The Hacker News data set ###\n",
    "opened_file = open('hacker_news.csv', encoding=\"utf-8\")\n",
    "read_file = reader(opened_file)\n",
    "hn = list(read_file)\n",
    "hn_header = hn[0]              #The header row\n",
    "hn_minus_header = hn[1:]       #The rest of the data\n",
    "\n",
    "#Set your values for the starting row and ending row of the dataset.\n",
    "starting_row = 0\n",
    "ending_row = 5\n",
    "total_rows = ending_row - starting_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below we created a function to explore the data, by slicing with a start row and end row. We also print the header row, the data in each row within the slice and the total number of rows and columns in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['id', 'title', 'url', 'num_points', 'num_comments', 'author', 'created_at']\n",
      "\n",
      "The first 5 rows of the dataset:\n",
      "\n",
      "['12224879', 'Interactive Dynamic Video', 'http://www.interactivedynamicvideo.com/', '386', '52', 'ne0phyte', '8/4/2016 11:52']\n",
      "\n",
      "\n",
      "['10975351', 'How to Use Open Source and Shut the Fuck Up at the Same Time', 'http://hueniverse.com/2016/01/26/how-to-use-open-source-and-shut-the-fuck-up-at-the-same-time/', '39', '10', 'josep2', '1/26/2016 19:30']\n",
      "\n",
      "\n",
      "['11964716', \"Florida DJs May Face Felony for April Fools' Water Joke\", 'http://www.thewire.com/entertainment/2013/04/florida-djs-april-fools-water-joke/63798/', '2', '1', 'vezycash', '6/23/2016 22:20']\n",
      "\n",
      "\n",
      "['11919867', 'Technology ventures: From Idea to Enterprise', 'https://www.amazon.com/Technology-Ventures-Enterprise-Thomas-Byers/dp/0073523429', '3', '1', 'hswarna', '6/17/2016 0:01']\n",
      "\n",
      "\n",
      "['10301696', 'Note by Note: The Making of Steinway L1037 (2007)', 'http://www.nytimes.com/2007/11/07/movies/07stein.html?_r=0', '8', '2', 'walterbell', '9/30/2015 4:12']\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Number of rows: 20100\n",
      "Number of columns: 7\n"
     ]
    }
   ],
   "source": [
    "def explore_data(dataset, start, end, rows_and_columns=False, newline=False):\n",
    "    dataset_slice = dataset[start:end]    \n",
    "    for row in dataset_slice:\n",
    "        print(row)\n",
    "        if newline == True:\n",
    "            print('\\n') # adds a new (empty) line between rows\n",
    "        \n",
    "    if rows_and_columns:\n",
    "        print('\\n')\n",
    "        print('Number of rows:', len(dataset))\n",
    "        print('Number of columns:', len(dataset[0]))\n",
    "\n",
    "print(hn_header)\n",
    "#print('\\n')\n",
    "print('\\nThe first ' + str(total_rows) + ' rows of the dataset:\\n')\n",
    "#print('\\n')\n",
    "explore_data(hn_minus_header, starting_row, ending_row, True, True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we will extract the posts for Ask HN and Show HN\n",
    "\n",
    "Below, we will separate the data into 3 different lists, print the size of each list and display the first few rows of ask_posts and show_posts:\n",
    "    * A list for Ask HN posts\n",
    "    * A list for Show HN posts\n",
    "    * A list for other posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Ask HN posts = 1744\n",
      "Length of Show HN posts = 1162\n",
      "Length of Other posts = 17194\n",
      "\n",
      "Here are 5 rows of Ask HN:\n",
      "['12296411', 'Ask HN: How to improve my personal website?', '', '2', '6', 'ahmedbaracat', '8/16/2016 9:55']\n",
      "['10610020', 'Ask HN: Am I the only one outraged by Twitter shutting down share counts?', '', '28', '29', 'tkfx', '11/22/2015 13:43']\n",
      "['11610310', 'Ask HN: Aby recent changes to CSS that broke mobile?', '', '1', '1', 'polskibus', '5/2/2016 10:14']\n",
      "['12210105', 'Ask HN: Looking for Employee #3 How do I do it?', '', '1', '3', 'sph130', '8/2/2016 14:20']\n",
      "['10394168', 'Ask HN: Someone offered to buy my browser extension from me. What now?', '', '28', '17', 'roykolak', '10/15/2015 16:38']\n",
      "\n",
      "Here are 5 rows of Show HN:\n",
      "['10627194', 'Show HN: Wio Link  ESP8266 Based Web of Things Hardware Development Platform', 'https://iot.seeed.cc', '26', '22', 'kfihihc', '11/25/2015 14:03']\n",
      "['10646440', 'Show HN: Something pointless I made', 'http://dn.ht/picklecat/', '747', '102', 'dhotson', '11/29/2015 22:46']\n",
      "['11590768', 'Show HN: Shanhu.io, a programming playground powered by e8vm', 'https://shanhu.io', '1', '1', 'h8liu', '4/28/2016 18:05']\n",
      "['12178806', 'Show HN: Webscope  Easy way for web developers to communicate with Clients', 'http://webscopeapp.com', '3', '3', 'fastbrick', '7/28/2016 7:11']\n",
      "['10872799', 'Show HN: GeoScreenshot  Easily test Geo-IP based web pages', 'https://www.geoscreenshot.com/', '1', '9', 'kpsychwave', '1/9/2016 20:45']\n"
     ]
    }
   ],
   "source": [
    "# Create 3 empty lists\n",
    "ask_posts = []\n",
    "show_posts = []\n",
    "other_posts = []\n",
    "\n",
    "for row in hn_minus_header:\n",
    "    title = row[1]\n",
    "    if title.lower().startswith('ask hn'):\n",
    "        ask_posts.append(row)\n",
    "    elif title.lower().startswith('show hn'):\n",
    "        show_posts.append(row)\n",
    "    else:\n",
    "        other_posts.append(row)\n",
    "        \n",
    "print('Length of Ask HN posts = ' + str(len(ask_posts)))\n",
    "print('Length of Show HN posts = ' + str(len(show_posts)))\n",
    "print('Length of Other posts = ' + str(len(other_posts)))\n",
    "\n",
    "print('\\nHere are 5 rows of Ask HN:')\n",
    "explore_data(ask_posts, starting_row, ending_row, False, False)\n",
    "print('\\nHere are 5 rows of Show HN:')\n",
    "explore_data(show_posts, starting_row, ending_row, False, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's determine if **ask posts** received the most comments or if **show posts** received the most comments, on average."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "14.038417431192661\n",
      "10.31669535283993\n"
     ]
    }
   ],
   "source": [
    "# Keep track of the total comments for Ask HN\n",
    "total_ask_comments = 0\n",
    "\n",
    "for row in ask_posts:\n",
    "    total_ask_comments += int(row[4])  \n",
    "    \n",
    "avg_ask_comments = total_ask_comments / len(ask_posts)\n",
    "print(avg_ask_comments)\n",
    "\n",
    "# Keep track of the total comments for Show HN\n",
    "total_show_comments = 0\n",
    "\n",
    "for row in show_posts:\n",
    "    total_show_comments += int(row[4])  \n",
    "    \n",
    "avg_show_comments = total_show_comments / len(show_posts)\n",
    "print(avg_show_comments)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis of above data points:\n",
    "* Ask HN has more posts than Show HN.\n",
    "* There are more posts for **Ask HN** and there are more comments on average vs **Show HN**. \n",
    "* We will explore **Ask HN** posts since this dataset has more posts and comments. \n",
    "\n",
    "### Next, we'll determine if ask posts created at a certain time are more likely to attract comments. \n",
    "We'll use the following steps to perform this analysis:\n",
    "\n",
    "* Calculate the amount of ask posts created in each hour of the day, along with the number of comments received.\n",
    "* Calculate the average number of comments ask posts receive by hour created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'09': 251, '13': 1253, '10': 793, '14': 1416, '16': 1814, '23': 543, '12': 687, '17': 1146, '15': 4477, '21': 1745, '20': 1722, '02': 1381, '18': 1439, '03': 421, '05': 464, '19': 1188, '01': 683, '22': 479, '08': 492, '04': 337, '00': 447, '06': 397, '07': 267, '11': 641}\n"
     ]
    }
   ],
   "source": [
    "import datetime as dt\n",
    "\n",
    "result_list = []\n",
    "for posts in ask_posts:\n",
    "    result_list.append([posts[6], int(posts[4])])\n",
    "    \n",
    "#Create 2 empty dictionaries\n",
    "counts_by_hour = {}\n",
    "comments_by_hour = {}\n",
    "\n",
    "#Loop through result_list\n",
    "date_format = \"%m/%d/%Y %H:%M\"\n",
    "\n",
    "for result in result_list:\n",
    "    created_date = result[0]\n",
    "    comment = result[1]\n",
    "    hour = dt.datetime.strptime(created_date, date_format).strftime(\"%H\")\n",
    "    \n",
    "    if hour not in counts_by_hour:\n",
    "        counts_by_hour[hour] = 1\n",
    "        comments_by_hour[hour] = comment\n",
    "    else:\n",
    "        counts_by_hour[hour] += 1\n",
    "        comments_by_hour[hour] += comment\n",
    "    \n",
    "print (comments_by_hour)      \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we'll use the **counts_by_hour** and **comments_by_hour** dictionaries to calculate the average number of comments for posts created during each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['09', 5.5777777777777775],\n",
       " ['13', 14.741176470588234],\n",
       " ['10', 13.440677966101696],\n",
       " ['14', 13.233644859813085],\n",
       " ['16', 16.796296296296298],\n",
       " ['23', 7.985294117647059],\n",
       " ['12', 9.41095890410959],\n",
       " ['17', 11.46],\n",
       " ['15', 38.5948275862069],\n",
       " ['21', 16.009174311926607],\n",
       " ['20', 21.525],\n",
       " ['02', 23.810344827586206],\n",
       " ['18', 13.20183486238532],\n",
       " ['03', 7.796296296296297],\n",
       " ['05', 10.08695652173913],\n",
       " ['19', 10.8],\n",
       " ['01', 11.383333333333333],\n",
       " ['22', 6.746478873239437],\n",
       " ['08', 10.25],\n",
       " ['04', 7.170212765957447],\n",
       " ['00', 8.127272727272727],\n",
       " ['06', 9.022727272727273],\n",
       " ['07', 7.852941176470588],\n",
       " ['11', 11.051724137931034]]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avg_comments_by_hour = []\n",
    "\n",
    "for hour in comments_by_hour:\n",
    "    avg_comments_by_hour.append([hour, comments_by_hour[hour] / counts_by_hour[hour]])\n",
    "    \n",
    "avg_comments_by_hour"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we will make the list above easier to read."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[5.5777777777777775, '09'], [14.741176470588234, '13'], [13.440677966101696, '10'], [13.233644859813085, '14'], [16.796296296296298, '16'], [7.985294117647059, '23'], [9.41095890410959, '12'], [11.46, '17'], [38.5948275862069, '15'], [16.009174311926607, '21'], [21.525, '20'], [23.810344827586206, '02'], [13.20183486238532, '18'], [7.796296296296297, '03'], [10.08695652173913, '05'], [10.8, '19'], [11.383333333333333, '01'], [6.746478873239437, '22'], [10.25, '08'], [7.170212765957447, '04'], [8.127272727272727, '00'], [9.022727272727273, '06'], [7.852941176470588, '07'], [11.051724137931034, '11']]\n",
      "\n",
      "\n",
      "[[38.5948275862069, '15'], [23.810344827586206, '02'], [21.525, '20'], [16.796296296296298, '16'], [16.009174311926607, '21'], [14.741176470588234, '13'], [13.440677966101696, '10'], [13.233644859813085, '14'], [13.20183486238532, '18'], [11.46, '17'], [11.383333333333333, '01'], [11.051724137931034, '11'], [10.8, '19'], [10.25, '08'], [10.08695652173913, '05'], [9.41095890410959, '12'], [9.022727272727273, '06'], [8.127272727272727, '00'], [7.985294117647059, '23'], [7.852941176470588, '07'], [7.796296296296297, '03'], [7.170212765957447, '04'], [6.746478873239437, '22'], [5.5777777777777775, '09']]\n"
     ]
    }
   ],
   "source": [
    "swap_avg_per_hour = []\n",
    "\n",
    "for hour in avg_comments_by_hour:\n",
    "    swap_avg_per_hour.append([hour[1], hour[0]])\n",
    "    \n",
    "print (swap_avg_per_hour)\n",
    "\n",
    "sorted_swap = sorted(swap_avg_per_hour, reverse=True)\n",
    "print ('\\n')\n",
    "print (sorted_swap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Top 5 Hours for Ask HN Post Comments:\n",
      "15:00: 38.59 average comments per post.\n",
      "02:00: 23.81 average comments per post.\n",
      "20:00: 21.52 average comments per post.\n",
      "16:00: 16.80 average comments per post.\n",
      "21:00: 16.01 average comments per post.\n"
     ]
    }
   ],
   "source": [
    "# Sort the values and print the 5 top hours with the highest average comments\n",
    "print ('\\nTop 5 Hours for Ask HN Post Comments:')\n",
    "\n",
    "for avg, hour in sorted_swap[:5]:\n",
    "    print(\n",
    "        \"{}: {:.2f} average comments per post.\".format(\n",
    "            dt.datetime.strptime(hour, \"%H\").strftime(\"%H:%M\"),avg\n",
    "        )\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to our sample dataset, if you were to post a question to **Ask HN** in the 15:00 (or 3pm) hour, you would get the most visibility for your post. The 15:00 hour has almost 62% more comments on average than the 02:00 hour.\n",
    "According to the [documentation](https://www.kaggle.com/hacker-news/hacker-news-posts/home), the timezone is in Eastern timezone. \n",
    "\n",
    "# Conclusion\n",
    "In this project, we created the following tasks:\n",
    "* Read in a sample size of 20,100 rows of the Hacker News dataset\n",
    "* Seperated out the posts between **Ask HN**, **Show HN** and **Other**\n",
    "* Determined that **Ask HN** had to largest number of posts\n",
    "* Determined the average number of comments per **Ask HN** and **Show HN**. **Ask HN** had the highest average number of comments.\n",
    "* We focused on the **Ask HN** posts and calculated the average number of comments per hour in a given day.\n",
    "* We sorted our **Ask HN** dataset by average number of comments, in ascending order. \n",
    "* Finally, we displayed the top 5 hours for **Ask HN** posts, based on average number of comments.\n",
    "\n",
    "Our recommendataion for getting the highest visibility of an **Ask HN** post is to create a post in the 15:00 to 15:59 Eastern timeframe. Note that we excluded posts that did not have any comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
